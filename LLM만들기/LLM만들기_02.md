```python
import requests
from bs4 import BeautifulSoup
import re

def get_text_from_korean_url(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    text = soup.get_text()
    # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥, íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^ê°€-í£0-9a-zA-Z .,?!]', '', text)
    return text
```


```python
from transformers import AutoTokenizer, GPT2LMHeadModel

model_name = "skt/kogpt2-base-v2"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

tokenizer.pad_token = tokenizer.eos_token
model.resize_token_embeddings(len(tokenizer), mean_resizing=False)

```




    Embedding(51201, 768)




```python
from datasets import Dataset

url = "https://ko.wikipedia.org/wiki/ì¸ê³µì§€ëŠ¥"
text_data = get_text_from_korean_url(url)

# Dataset í˜•íƒœë¡œ êµ¬ì„±
dataset = Dataset.from_dict({"text": [text_data]})
```


```python
def tokenize_function(example):
    tokens = tokenizer(example["text"], truncation=True, padding="max_length", max_length=512)
    tokens["labels"] = tokens["input_ids"].copy()
    return tokens

tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
```


    Map:   0%|          | 0/1 [00:00<?, ? examples/s]



```python
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./Result02",
    per_device_train_batch_size=1,
    num_train_epochs=3,
    logging_steps=10,
    save_steps=500,
    save_total_limit=1,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset
)

trainer.train()

```

    `loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
    



    
      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [3/3 00:06, Epoch 3/3]
    




    TrainOutput(global_step=3, training_loss=4.529757817586263, metrics={'train_runtime': 7.5884, 'train_samples_per_second': 0.395, 'train_steps_per_second': 0.395, 'total_flos': 783876096000.0, 'train_loss': 4.529757817586263, 'epoch': 3.0})




```python
model.eval()

prompt = "ì¸ê³µì§€ëŠ¥ì´ë€"  # ì‹œì‘ ë¬¸ì¥
input_ids = tokenizer.encode(prompt, return_tensors="pt")

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device = "cpu"
model.to(device)
input_ids = input_ids.to(device) 

generated_ids = model.generate(
    input_ids,
    max_length=50,
    num_return_sequences=5,
    do_sample=True,
    top_k=50,
    top_p=0.95,
    temperature=0.9,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id
)

print("\nğŸ“Œ ìƒì„±ëœ ë¬¸ì¥ 5ê°œ:")
for i, gen_id in enumerate(generated_ids):
    text = tokenizer.decode(gen_id, skip_special_tokens=True)
    print(f"{i+1}. {text}\n")
```

    
    ğŸ“Œ ìƒì„±ëœ ë¬¸ì¥ 5ê°œ:
    1. ì¸ê³µì§€ëŠ¥ì´ë€ ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ ì–´ëŠ ì •ë„ ì´í•´í–ˆëŠ”ì§€, ì•„ë‹ˆë©´ ì¸ê³µì§€ëŠ¥ì— ëŒ€í•œ ì´í•´ ìˆ˜ì¤€ì´ ì–´ëŠ ì •ë„ì¸ì§€ íŒŒì•…í•˜ê¸° ìœ„í•´ ì—°êµ¬ìë“¤ì„ ì°¾ëŠ” ê²ƒ ì—­ì‹œ ì‰¬ìš´ ì¼ì´ ì•„ë‹ˆë‹¤.
    ë‹¤ë§Œ ì´ëŸ¬í•œ ì¼ì€ ì—°êµ¬ìë“¤ì´ ì—°êµ¬ í™œë™ì„ ì‹œì‘í•  ë•Œ ë°˜ë“œì‹œ í•´ì•¼ í•˜ëŠ” ì¼ì´ë¯€ë¡œ,
    
    2. ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€?
    ì•„ë§ˆ í”„ë¡œê·¸ë˜ë° ì–¸ì–´í•™ì—ì„œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´í•™ê³¼ëŠ” ê·¸ ì •ì˜ì ì¸ ë¶€ë¶„, ì¦‰ ëª¨ë“  ì‚¬ìš©ìì— ëŒ€í•œ ì»´í“¨í„°ì  ì´í•´ì™€ ê´€ë ¨ëœ ê°€ì¥ ê°•ë ¥í•œ ê°œë… ì¤‘ í•˜ë‚˜ë¡œ ì¸ì‹ë˜ê³  ìˆë‹¤.
    í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ì—°êµ¬í•˜ëŠ” ì‚¬ëŒë“¤ì€ ê·¸ ê°œë…ë“¤ì„ ì–´ë–»ê²Œ
    
    3. ì¸ê³µì§€ëŠ¥ì´ë€ê²Œ ë­˜ê¹Œìš”?
    ê·¸ì € ë§í•˜ë ¤ëŠ” ê·¸ ì´ìœ ì™€ ë‹µì€ ë‹ˆë‹¤.
    ì‚¬ì‹¤ ì´ë¼ëŠ” ì´ ì¸ê³µì§€ëŠ¥ì€ ì¼ì¢…ì˜ ë§ˆì´ë„ˆë¦¬í‹°ë¥¼ ë‹¤ë£¨ì—ˆê³  ì´ë¼ëŠ” ê°œë…ì„ ì™„ì „íˆ ë¬´ì‹œí–ˆë‹¤ëŠ” ê±°ì£ .
    ê·¸ë˜ì„œ ë§ˆ
    
    4. ì¸ê³µì§€ëŠ¥ì´ë€ ë§ì´ë‹¤. ì§€ë‚œ 2000ë…„ ì´ˆìˆœë¶€í„° ì´ë‹¬ ì´ˆê¹Œì§€ ë¯¸êµ­ ì‹¤ë¦¬ì½˜ë°¸ë¦¬ì˜ ITê¸°ì—… ìµœê³ ê²½ì˜ì(CEO)ë“¤ì´ ê°€ì¥ ì„ í˜¸í•˜ëŠ” íˆ¬ìí•­ëª©ì˜ í•˜ë‚˜ë¡œ ê¼½í˜”ë˜ ê²ƒì´ ë°”ë¡œ 'ê¸°ìˆ 'ê³¼ 'ê¸°ì—…'ì´ì—ˆë‹¤.
    ì§€ë‚œ 2004ë…„ì˜
    
    5. ì¸ê³µì§€ëŠ¥ì´ë€ ì–´ë–¤ ê²ƒì¸ì§€, ê·¸ë¦¬ê³  ê·¸ê²Œ ì–´ë–¤ ê²ƒì¸ì§€ ë“±ë“±ì€ ì—¬ì „íˆ ìˆ˜ìˆ˜ê»˜ë¼ë¡œ ë‚¨ì•„ ìˆë‹¤.
    ê·¸ëŸ¬ë‚˜ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ì–´ë–¤ ë‹¤ë¥¸ ë„êµ¬ë“¤ì€ ì–´ë–¤ ê²ƒë“¤ì´ë“ ì§€ ê°„ì—, ê·¸ê±¸ ì•Œê³  ìˆëŠ” í•œ ê·¸ë“¤ì€ ì•„ì£¼ ì˜¤ë˜ì „ë¶€í„° ê·¸ëŸ° ê²ƒì´ ê°€ëŠ¥í•  ê²ƒì´ë¼ëŠ” ìƒê°ì„ í•´ì™”ë‹¤.
    ì–´ë–¤ ì‚¬ëŒë“¤ì€ ìì‹ ì´ í•œ
    
    
